{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MDE5KTCZAmDY",
        "iz7eyilI1YHd",
        "R9WiuiS81WUY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62c38a1761eb4a569a2ed6e353420044": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e662b7f7c7b940e5be385d47d0a84654",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Applying Weight Compression \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m156/156\u001b[0m • \u001b[38;2;0;104;181m0:00:13\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Applying Weight Compression <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">156/156</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:13</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "e662b7f7c7b940e5be385d47d0a84654": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "🟢 **Tiny LLaMa**"
      ],
      "metadata": {
        "id": "XxcJjV5H8vWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install openvino and optimum intel**"
      ],
      "metadata": {
        "id": "HgEdFM839Ol4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q02Tj3mBvBfY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "feff3c11-6a7f-4120-b7f3-7be7e9d1aac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping openvino as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping openvino-dev as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping openvino-nightly as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optimum as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optimum-intel as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall -q -y openvino openvino-dev openvino-nightly optimum optimum-intel\n",
        "%pip install -q openvino-nightly \"nncf>=2.7\" \"transformers>=4.36.0\" onnx \"optimum>=1.16.1\" \"accelerate\" \"datasets\" gradio \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A terminal to measure CPU usage**\n",
        "\n",
        "Using nmon framework to visualise CPU usage on the terminal"
      ],
      "metadata": {
        "id": "8V4bYop_1DG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ygX3nzsZ0yli",
        "outputId": "e297ff14-106c-4c55-a65a-619372049bcb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab-xterm in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.10/dist-packages (from colab-xterm) (6.3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FP32 default model datatype"
      ],
      "metadata": {
        "id": "MDE5KTCZAmDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and convert the model into OV IR format.**\n",
        "\n",
        "The weight format is set to FP32, so no compression is taking place"
      ],
      "metadata": {
        "id": "L2C1fXOtxNBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export openvino --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format fp32 ov_model_fp32_tinyllama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LPs43Ic8ymr-",
        "outputId": "4dd064a2-8165-47b7-f4e0-4097bd47f39e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-10 17:01:48.198206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-10 17:01:48.198277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-10 17:01:48.200515: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-10 17:01:50.327126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "Framework not specified. Using pt to export the model.\n",
            "config.json: 100% 608/608 [00:00<00:00, 3.29MB/s]\n",
            "model.safetensors: 100% 2.20G/2.20G [00:18<00:00, 120MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 708kB/s]\n",
            "Automatic task detection to text-generation-with-past (possible synonyms are: causal-lm-with-past).\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 5.92MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 244MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 9.79MB/s]\n",
            "special_tokens_map.json: 100% 551/551 [00:00<00:00, 3.21MB/s]\n",
            "Using the export variant default. Available variants are:\n",
            "    - default: The default ONNX variant.\n",
            "Using framework PyTorch: 2.1.0+cu121\n",
            "WARNING:root:\u001b[1;31m[WARNING] For good performance with stateful models, transformers>=4.36.2 and PyTorch>=2.1.1 are required. This Python environment has Transformers 4.38.2 and PyTorch 2.1.0+cu121. Consider upgrading PyTorch and Transformers, for example by running `pip install --upgrade --upgrade-strategy eager optimum[openvino]`, and export the model again\u001b[0m\n",
            "WARNING:root:Cannot apply model.to_bettertransformer because of the exception:\n",
            "Transformers now supports natively BetterTransformer optimizations (torch.nn.functional.scaled_dot_product_attention) for the model type llama. As such, there is no need to use `model.to_bettertransformers()` or `BetterTransformer.transform(model)` from the Optimum library. Please upgrade to transformers>=4.36 and torch>=2.1.1 to use it. Details: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention.. Usage model with stateful=True may be non-effective if model does not contain torch.functional.scaled_dot_product_attention\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
            "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1068: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if seq_length > self.causal_mask.shape[-1]:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:385: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
            "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
            "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
            "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the size of the FP32 model**"
      ],
      "metadata": {
        "id": "euWI9o0NxMFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "fp32_model_dir = Path(\"/content/ov_model_fp32_tinyllama\")\n",
        "fp32_weights = fp32_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "\n",
        "if fp32_weights.exists():\n",
        "    print(f\"SIZE OF THE DEFAULT MODEL WITH FP32 WIIGHTS IS {fp32_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vfX3Cv2bzULF",
        "outputId": "b283d675-06bd-450f-d215-16504687ea15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIZE OF THE DEFAULT MODEL WITH FP32 WIIGHTS IS 4200.35 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create an OV object for use in generation**\n",
        "\n"
      ],
      "metadata": {
        "id": "8U7pk31W0FC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.intel.openvino import OVModelForCausalLM, OVWeightQuantizationConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_dir = fp32_model_dir\n",
        "\n",
        "print(f\"Loading model from {model_dir}\")\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device=\"CPU\",\n",
        "    ov_config=ov_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4A6AfQcJkTHY",
        "outputId": "dad907d7-0ced-49eb-8e81-8bbc4cba5ce0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/ov_model_fp32_tinyllama\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compiling the model to CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Helper Functions for Generation`**"
      ],
      "metadata": {
        "id": "Qo1MHfIf1ZsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from time import perf_counter\n",
        "from typing import List\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, TextIteratorStreamer\n",
        "import numpy as np\n",
        "\n",
        "model_configuration = {\n",
        "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"prompt_template\":  \"<|user|>\\n{instruction}</s> \\n<|assistant|>\\n\",\n",
        "        \"tokenizer_kwargs\": {\"add_special_tokens\": False},\n",
        "    }\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "\n",
        "\n",
        "def get_special_token_id(tokenizer: AutoTokenizer, key: str) -> int:\n",
        "    \"\"\"\n",
        "    Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
        "        key (str): the key to convert to a single token\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: if more than one ID was generated\n",
        "\n",
        "    Returns:\n",
        "        int: the token ID for the given key\n",
        "    \"\"\"\n",
        "    token_ids = tokenizer.encode(key)\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
        "    return token_ids[0]\n",
        "\n",
        "response_key = model_configuration.get(\"response_key\")\n",
        "tokenizer_response_key = None\n",
        "\n",
        "if response_key is not None:\n",
        "    tokenizer_response_key = next((token for token in tokenizer.additional_special_tokens if token.startswith(response_key)), None)\n",
        "\n",
        "end_key_token_id = None\n",
        "if tokenizer_response_key:\n",
        "    try:\n",
        "        end_key = model_configuration.get(\"end_key\")\n",
        "        if end_key:\n",
        "            end_key_token_id = get_special_token_id(tokenizer, end_key)\n",
        "        # Ensure generation stops once it generates \"### End\"\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "prompt_template = model_configuration.get(\"prompt_template\", \"{instruction}\")\n",
        "end_key_token_id = end_key_token_id or tokenizer.eos_token_id\n",
        "pad_token_id = end_key_token_id or tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "UcP429JQaggB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "aa3670f7-e521-451e-81b3-03f1abb74ddf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def estimate_latency(current_time:float, current_perf_text:str, new_gen_text:str, per_token_time:List[float], num_tokens:int):\n",
        "    \"\"\"\n",
        "    Helper function for performance estimation\n",
        "\n",
        "    Parameters:\n",
        "      current_time (float): This step time in seconds.\n",
        "      current_perf_text (str): Current content of performance UI field.\n",
        "      new_gen_text (str): New generated text.\n",
        "      per_token_time (List[float]): history of performance from previous steps.\n",
        "      num_tokens (int): Total number of generated tokens.\n",
        "\n",
        "    Returns:\n",
        "      update for performance text field\n",
        "      update for a total number of tokens\n",
        "    \"\"\"\n",
        "    # start = time.time()\n",
        "    num_current_toks = len(tokenizer.encode(new_gen_text))\n",
        "    num_tokens += num_current_toks\n",
        "    per_token_time.append(num_current_toks / current_time)\n",
        "    if len(per_token_time) > 10 and len(per_token_time) % 4 == 0:\n",
        "        current_bucket = per_token_time[:-10]\n",
        "        # end = time.time()\n",
        "        logging.critical(f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}.\")\n",
        "        return f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}.\", num_tokens\n",
        "    # current_perf_text = f\"Average generation speed: {np.mean(per_token_time):.2f} tokens/s. Total generated tokens: {num_tokens}\"\n",
        "    return current_perf_text, num_tokens"
      ],
      "metadata": {
        "id": "oJ4VnhlwamKz"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation Function**"
      ],
      "metadata": {
        "id": "Yj4sCKcE0Zoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "import logging\n",
        "import time\n",
        "def run_generation(user_text:str, chat_history:str):\n",
        "    \"\"\"\n",
        "    Text generation function\n",
        "\n",
        "    Parameters:\n",
        "      user_text (str): User-provided instruction for a generation.\n",
        "      top_p (float):  Nucleus sampling. If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for a generation.\n",
        "      temperature (float): The value used to module the logits distribution.\n",
        "      top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "      max_new_tokens (int): Maximum length of generated sequence.\n",
        "      perf_text (str): Content of text field for printing performance results.\n",
        "    Returns:\n",
        "      model_output (str) - model-generated text\n",
        "      perf_text (str) - updated perf text filed content\n",
        "    \"\"\"\n",
        "    # if user_text == \"Hello\" or user_text == \"hello\":\n",
        "    #   return \"Hello!! Nice to see you here. I can follow instructions and generate text for you.\", \"Will show average generation speed and number of tokens generated here.\"\n",
        "\n",
        "    # Prepare input prompt according to model expected template\n",
        "    prompt_text = prompt_template.format(instruction=user_text)\n",
        "\n",
        "    # Tokenize the user text.\n",
        "    model_inputs = tokenizer(prompt_text, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "\n",
        "    # Start generation on a separate thread, so that we don't block the UI. The text is pulled from the streamer\n",
        "    # in the main thread. Adds timeout to the streamer to handle exceptions in the generation thread.\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        temperature=float(0.8),\n",
        "        top_k=0,\n",
        "        eos_token_id=end_key_token_id,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "    t = Thread(target=ov_model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    # Pull the generated text from the streamer, and update the model output.\n",
        "    model_output = \"\"\n",
        "    per_token_time = []\n",
        "    num_tokens = 0\n",
        "    start = perf_counter()\n",
        "    start = time.time()\n",
        "    for new_text in streamer:\n",
        "        current_time = perf_counter() - start\n",
        "        model_output += new_text\n",
        "        perf_text, num_tokens = estimate_latency(current_time, \"\", new_text, per_token_time, num_tokens)\n",
        "        yield model_output\n",
        "        start = perf_counter()\n",
        "    end = time.time()\n",
        "    logging.critical(f\"Inference time is {end-start} seconds\")\n",
        "    chat_history.append((user_text, model_output))\n",
        "    return model_output, chat_history"
      ],
      "metadata": {
        "id": "SfdDCzINapSC"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**"
      ],
      "metadata": {
        "id": "quXP_jJM1VpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ZnbIwV-kth",
        "outputId": "0fdcb516-4dbb-46e0-fc52-09b835c40cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%xterm` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  gr.ChatInterface(run_generation)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue()\n",
        "    try:\n",
        "        demo.launch(height=800, debug = True)\n",
        "    except Exception:\n",
        "        demo.launch(share=True, height=800, debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1912
        },
        "id": "24VhEboUlh8F",
        "outputId": "4c22efa8-ee11-4c1c-e3cf-8a00690f8c19"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://4db142e5aee8c661fb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4db142e5aee8c661fb.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CRITICAL:root:Average generation speed: 1.09 tokens/s. Total generated tokens: 31.\n",
            "CRITICAL:root:Average generation speed: 4.10 tokens/s. Total generated tokens: 39.\n",
            "CRITICAL:root:Average generation speed: 5.19 tokens/s. Total generated tokens: 48.\n",
            "CRITICAL:root:Average generation speed: 4.84 tokens/s. Total generated tokens: 61.\n",
            "CRITICAL:root:Average generation speed: 5.36 tokens/s. Total generated tokens: 71.\n",
            "CRITICAL:root:Average generation speed: 5.24 tokens/s. Total generated tokens: 81.\n",
            "CRITICAL:root:Average generation speed: 5.19 tokens/s. Total generated tokens: 90.\n",
            "CRITICAL:root:Average generation speed: 5.28 tokens/s. Total generated tokens: 103.\n",
            "CRITICAL:root:Average generation speed: 5.32 tokens/s. Total generated tokens: 114.\n",
            "CRITICAL:root:Average generation speed: 5.40 tokens/s. Total generated tokens: 126.\n",
            "CRITICAL:root:Average generation speed: 5.47 tokens/s. Total generated tokens: 138.\n",
            "CRITICAL:root:Average generation speed: 5.58 tokens/s. Total generated tokens: 150.\n",
            "CRITICAL:root:Average generation speed: 5.66 tokens/s. Total generated tokens: 160.\n",
            "CRITICAL:root:Average generation speed: 5.69 tokens/s. Total generated tokens: 169.\n",
            "CRITICAL:root:Average generation speed: 5.67 tokens/s. Total generated tokens: 180.\n",
            "CRITICAL:root:Average generation speed: 5.55 tokens/s. Total generated tokens: 191.\n",
            "CRITICAL:root:Average generation speed: 5.58 tokens/s. Total generated tokens: 203.\n",
            "CRITICAL:root:Average generation speed: 5.61 tokens/s. Total generated tokens: 214.\n",
            "CRITICAL:root:Average generation speed: 5.69 tokens/s. Total generated tokens: 226.\n",
            "CRITICAL:root:Average generation speed: 5.73 tokens/s. Total generated tokens: 237.\n",
            "CRITICAL:root:Average generation speed: 5.72 tokens/s. Total generated tokens: 248.\n",
            "CRITICAL:root:Average generation speed: 5.71 tokens/s. Total generated tokens: 258.\n",
            "CRITICAL:root:Average generation speed: 5.69 tokens/s. Total generated tokens: 268.\n",
            "CRITICAL:root:Average generation speed: 5.67 tokens/s. Total generated tokens: 279.\n",
            "CRITICAL:root:Average generation speed: 5.63 tokens/s. Total generated tokens: 287.\n",
            "CRITICAL:root:Average generation speed: 5.56 tokens/s. Total generated tokens: 298.\n",
            "CRITICAL:root:Average generation speed: 5.57 tokens/s. Total generated tokens: 307.\n",
            "CRITICAL:root:Average generation speed: 5.59 tokens/s. Total generated tokens: 317.\n",
            "CRITICAL:root:Average generation speed: 5.51 tokens/s. Total generated tokens: 330.\n",
            "CRITICAL:root:Average generation speed: 5.54 tokens/s. Total generated tokens: 334.\n",
            "CRITICAL:root:Average generation speed: 5.55 tokens/s. Total generated tokens: 348.\n",
            "CRITICAL:root:Average generation speed: 5.51 tokens/s. Total generated tokens: 360.\n",
            "CRITICAL:root:Average generation speed: 5.41 tokens/s. Total generated tokens: 372.\n",
            "CRITICAL:root:Average generation speed: 5.53 tokens/s. Total generated tokens: 381.\n",
            "CRITICAL:root:Average generation speed: 5.42 tokens/s. Total generated tokens: 389.\n",
            "CRITICAL:root:Average generation speed: 5.42 tokens/s. Total generated tokens: 401.\n",
            "CRITICAL:root:Average generation speed: 5.41 tokens/s. Total generated tokens: 412.\n",
            "CRITICAL:root:Average generation speed: 5.32 tokens/s. Total generated tokens: 423.\n",
            "CRITICAL:root:Average generation speed: 5.39 tokens/s. Total generated tokens: 434.\n",
            "CRITICAL:root:Average generation speed: 5.37 tokens/s. Total generated tokens: 442.\n",
            "CRITICAL:root:Average generation speed: 5.36 tokens/s. Total generated tokens: 451.\n",
            "CRITICAL:root:Average generation speed: 5.34 tokens/s. Total generated tokens: 458.\n",
            "CRITICAL:root:Average generation speed: 5.32 tokens/s. Total generated tokens: 472.\n",
            "CRITICAL:root:Average generation speed: 5.34 tokens/s. Total generated tokens: 482.\n",
            "CRITICAL:root:Average generation speed: 5.35 tokens/s. Total generated tokens: 493.\n",
            "CRITICAL:root:Average generation speed: 5.35 tokens/s. Total generated tokens: 505.\n",
            "CRITICAL:root:Average generation speed: 5.33 tokens/s. Total generated tokens: 515.\n",
            "CRITICAL:root:Average generation speed: 5.33 tokens/s. Total generated tokens: 524.\n",
            "CRITICAL:root:Average generation speed: 5.34 tokens/s. Total generated tokens: 533.\n",
            "CRITICAL:root:Inference time is 1710089888.1860693 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7873 <> https://4db142e5aee8c661fb.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INT 8 - compressed model\n"
      ],
      "metadata": {
        "id": "iz7eyilI1YHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and convert the model into OV IR format.**\n",
        "\n",
        "The weight format is set to INT8. We are using Optimum Intel library and the OVModelForCausalLM class to get the compressed and converted model from Huggingface.\n"
      ],
      "metadata": {
        "id": "SAFyVMIT14b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from optimum.intel.openvino import OVModelForCausalLM, OVWeightQuantizationConfig\n",
        "int8_model_dir = Path(\"/content/ov_model_lib_int8_tinyllama\")\n",
        "ov_model_lib = OVModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", export=True, compile=False, load_in_8bit=True)\n",
        "ov_model_lib.save_pretrained(int8_model_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821,
          "referenced_widgets": [
            "62c38a1761eb4a569a2ed6e353420044",
            "e662b7f7c7b940e5be385d47d0a84654"
          ]
        },
        "id": "YZMe1KCm1gg_",
        "outputId": "808d3d3a-a301-4cd1-c842-ca986ce9d327"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Framework not specified. Using pt to export the model.\n",
            "Using the export variant default. Available variants are:\n",
            "    - default: The default ONNX variant.\n",
            "The model weights will be quantized to int8.\n",
            "Using framework PyTorch: 2.1.0+cu121\n",
            "WARNING:root:\u001b[1;31m[WARNING] For good performance with stateful models, transformers>=4.36.2 and PyTorch>=2.1.1 are required. This Python environment has Transformers 4.38.2 and PyTorch 2.1.0+cu121. Consider upgrading PyTorch and Transformers, for example by running `pip install --upgrade --upgrade-strategy eager optimum[openvino]`, and export the model again\u001b[0m\n",
            "WARNING:root:Cannot apply model.to_bettertransformer because of the exception:\n",
            "Transformers now supports natively BetterTransformer optimizations (torch.nn.functional.scaled_dot_product_attention) for the model type llama. As such, there is no need to use `model.to_bettertransformers()` or `BetterTransformer.transform(model)` from the Optimum library. Please upgrade to transformers>=4.36 and torch>=2.1.1 to use it. Details: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention.. Usage model with stateful=True may be non-effective if model does not contain torch.functional.scaled_dot_product_attention\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
            "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1068: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if seq_length > self.causal_mask.shape[-1]:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:385: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
            "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
            "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
            "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:Statistics of the bitwidth distribution:\n",
            "+--------------+---------------------------+-----------------------------------+\n",
            "| Num bits (N) | % all parameters (layers) |    % ratio-defining parameters    |\n",
            "|              |                           |             (layers)              |\n",
            "+==============+===========================+===================================+\n",
            "| 8            | 100% (156 / 156)          | 100% (156 / 156)                  |\n",
            "+--------------+---------------------------+-----------------------------------+\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62c38a1761eb4a569a2ed6e353420044"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:Statistics of the bitwidth distribution:\n",
            "+--------------+---------------------------+-----------------------------------+\n",
            "| Num bits (N) | % all parameters (layers) |    % ratio-defining parameters    |\n",
            "|              |                           |             (layers)              |\n",
            "+==============+===========================+===================================+\n",
            "+--------------+---------------------------+-----------------------------------+\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /content/ov_model_lib_int8_tinyllama/openvino_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the size of the INT8 compressed model**"
      ],
      "metadata": {
        "id": "T86_kbTx2JmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "int8_model_dir = Path(\"/content/ov_model_lib_int8_tinyllama\")\n",
        "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "\n",
        "if int8_weights.exists():\n",
        "    print(\n",
        "        f\"SIZE OF THE COMPRESSED MODEL WITH INT8 WEIGHTS IS  {int8_weights.stat().st_size / 1024 / 1024:.2f} MB\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U-_pIu-kvxd9",
        "outputId": "b14aa30e-fcfc-4130-ed23-c19dad4e6f17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIZE OF THE COMPRESSED MODEL WITH INT8 WEIGHTS IS  1055.54 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper Functions for Generation**"
      ],
      "metadata": {
        "id": "ZI-rlK_Sf4VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from time import perf_counter\n",
        "from typing import List\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, TextIteratorStreamer\n",
        "import numpy as np\n",
        "\n",
        "model_configuration = {\n",
        "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"prompt_template\":  \"<|user|>\\n{instruction}</s> \\n<|assistant|>\\n\",\n",
        "        \"tokenizer_kwargs\": {\"add_special_tokens\": False},\n",
        "    }\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "\n",
        "\n",
        "def get_special_token_id(tokenizer: AutoTokenizer, key: str) -> int:\n",
        "    \"\"\"\n",
        "    Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
        "        key (str): the key to convert to a single token\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: if more than one ID was generated\n",
        "\n",
        "    Returns:\n",
        "        int: the token ID for the given key\n",
        "    \"\"\"\n",
        "    token_ids = tokenizer.encode(key)\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
        "    return token_ids[0]\n",
        "\n",
        "response_key = model_configuration.get(\"response_key\")\n",
        "tokenizer_response_key = None\n",
        "\n",
        "if response_key is not None:\n",
        "    tokenizer_response_key = next((token for token in tokenizer.additional_special_tokens if token.startswith(response_key)), None)\n",
        "\n",
        "end_key_token_id = None\n",
        "if tokenizer_response_key:\n",
        "    try:\n",
        "        end_key = model_configuration.get(\"end_key\")\n",
        "        if end_key:\n",
        "            end_key_token_id = get_special_token_id(tokenizer, end_key)\n",
        "        # Ensure generation stops once it generates \"### End\"\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "prompt_template = model_configuration.get(\"prompt_template\", \"{instruction}\")\n",
        "end_key_token_id = end_key_token_id or tokenizer.eos_token_id\n",
        "pad_token_id = end_key_token_id or tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "yYRL77sNyS59"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def estimate_latency(current_time:float, current_perf_text:str, new_gen_text:str, per_token_time:List[float], num_tokens:int):\n",
        "    \"\"\"\n",
        "    Helper function for performance estimation\n",
        "\n",
        "    Parameters:\n",
        "      current_time (float): This step time in seconds.\n",
        "      current_perf_text (str): Current content of performance UI field.\n",
        "      new_gen_text (str): New generated text.\n",
        "      per_token_time (List[float]): history of performance from previous steps.\n",
        "      num_tokens (int): Total number of generated tokens.\n",
        "\n",
        "    Returns:\n",
        "      update for performance text field\n",
        "      update for a total number of tokens\n",
        "    \"\"\"\n",
        "    # start = time.time()\n",
        "    num_current_toks = len(tokenizer.encode(new_gen_text))\n",
        "    num_tokens += num_current_toks\n",
        "    per_token_time.append(num_current_toks / current_time)\n",
        "    if len(per_token_time) > 10 and len(per_token_time) % 4 == 0:\n",
        "        current_bucket = per_token_time[:-10]\n",
        "        # end = time.time()\n",
        "        logging.critical(f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}.\")\n",
        "        return f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}.\", num_tokens\n",
        "    # current_perf_text = f\"Average generation speed: {np.mean(per_token_time):.2f} tokens/s. Total generated tokens: {num_tokens}\"\n",
        "    return current_perf_text, num_tokens"
      ],
      "metadata": {
        "id": "TSJAcR8cEBG9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation function**"
      ],
      "metadata": {
        "id": "EZobRBm42LBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "import logging\n",
        "import time\n",
        "def run_generation(user_text:str, chat_history:str):\n",
        "    \"\"\"\n",
        "    Text generation function\n",
        "\n",
        "    Parameters:\n",
        "      user_text (str): User-provided instruction for a generation.\n",
        "      top_p (float):  Nucleus sampling. If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for a generation.\n",
        "      temperature (float): The value used to module the logits distribution.\n",
        "      top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "      max_new_tokens (int): Maximum length of generated sequence.\n",
        "      perf_text (str): Content of text field for printing performance results.\n",
        "    Returns:\n",
        "      model_output (str) - model-generated text\n",
        "      perf_text (str) - updated perf text filed content\n",
        "    \"\"\"\n",
        "    # if user_text == \"Hello\" or user_text == \"hello\":\n",
        "    #   return \"Hello!! Nice to see you here. I can follow instructions and generate text for you.\", \"Will show average generation speed and number of tokens generated here.\"\n",
        "\n",
        "    # Prepare input prompt according to model expected template\n",
        "    prompt_text = prompt_template.format(instruction=user_text)\n",
        "\n",
        "    # Tokenize the user text.\n",
        "    model_inputs = tokenizer(prompt_text, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "\n",
        "    # Start generation on a separate thread, so that we don't block the UI. The text is pulled from the streamer\n",
        "    # in the main thread. Adds timeout to the streamer to handle exceptions in the generation thread.\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        temperature=float(0.8),\n",
        "        top_k=0,\n",
        "        eos_token_id=end_key_token_id,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "    t = Thread(target=ov_model_lib.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    # Pull the generated text from the streamer, and update the model output.\n",
        "    model_output = \"\"\n",
        "    per_token_time = []\n",
        "    num_tokens = 0\n",
        "    start = perf_counter()\n",
        "    start = time.time()\n",
        "    for new_text in streamer:\n",
        "        current_time = perf_counter() - start\n",
        "        model_output += new_text\n",
        "        perf_text, num_tokens = estimate_latency(current_time, \"\", new_text, per_token_time, num_tokens)\n",
        "        yield model_output\n",
        "        start = perf_counter()\n",
        "    end = time.time()\n",
        "    logging.critical(f\"Inference time is {end-start} seconds\")\n",
        "    chat_history.append((user_text, model_output))\n",
        "    return model_output, chat_history"
      ],
      "metadata": {
        "id": "8wUHia-3EFQU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference result**"
      ],
      "metadata": {
        "id": "_fN53aWogLdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "08IeGIp6CCBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  gr.ChatInterface(run_generation)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue()\n",
        "    try:\n",
        "        demo.launch(debug = True)\n",
        "    except Exception:\n",
        "        demo.launch(share=True, debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2127
        },
        "id": "M0vtWE22E3sH",
        "outputId": "a52a9923-c4a2-4af0-e633-1fca3759dc61"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://4113ef58752bd4babd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4113ef58752bd4babd.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compiling the model to CPU ...\n",
            "CRITICAL:root:Average generation speed: 1.58 tokens/s. Total generated tokens: 32.\n",
            "CRITICAL:root:Average generation speed: 10.21 tokens/s. Total generated tokens: 42.\n",
            "CRITICAL:root:Average generation speed: 11.77 tokens/s. Total generated tokens: 55.\n",
            "CRITICAL:root:Average generation speed: 11.94 tokens/s. Total generated tokens: 66.\n",
            "CRITICAL:root:Average generation speed: 11.21 tokens/s. Total generated tokens: 75.\n",
            "CRITICAL:root:Average generation speed: 10.61 tokens/s. Total generated tokens: 87.\n",
            "CRITICAL:root:Average generation speed: 10.36 tokens/s. Total generated tokens: 93.\n",
            "CRITICAL:root:Average generation speed: 10.56 tokens/s. Total generated tokens: 102.\n",
            "CRITICAL:root:Average generation speed: 10.85 tokens/s. Total generated tokens: 115.\n",
            "CRITICAL:root:Average generation speed: 10.78 tokens/s. Total generated tokens: 123.\n",
            "CRITICAL:root:Average generation speed: 10.17 tokens/s. Total generated tokens: 137.\n",
            "CRITICAL:root:Average generation speed: 10.94 tokens/s. Total generated tokens: 149.\n",
            "CRITICAL:root:Average generation speed: 10.97 tokens/s. Total generated tokens: 159.\n",
            "CRITICAL:root:Average generation speed: 11.17 tokens/s. Total generated tokens: 167.\n",
            "CRITICAL:root:Average generation speed: 11.19 tokens/s. Total generated tokens: 177.\n",
            "CRITICAL:root:Average generation speed: 11.35 tokens/s. Total generated tokens: 188.\n",
            "CRITICAL:root:Average generation speed: 11.35 tokens/s. Total generated tokens: 198.\n",
            "CRITICAL:root:Average generation speed: 10.95 tokens/s. Total generated tokens: 202.\n",
            "CRITICAL:root:Average generation speed: 10.98 tokens/s. Total generated tokens: 219.\n",
            "CRITICAL:root:Average generation speed: 10.65 tokens/s. Total generated tokens: 231.\n",
            "CRITICAL:root:Average generation speed: 10.62 tokens/s. Total generated tokens: 242.\n",
            "CRITICAL:root:Average generation speed: 10.72 tokens/s. Total generated tokens: 253.\n",
            "CRITICAL:root:Average generation speed: 10.77 tokens/s. Total generated tokens: 264.\n",
            "CRITICAL:root:Average generation speed: 10.86 tokens/s. Total generated tokens: 268.\n",
            "CRITICAL:root:Average generation speed: 10.81 tokens/s. Total generated tokens: 283.\n",
            "CRITICAL:root:Average generation speed: 10.78 tokens/s. Total generated tokens: 293.\n",
            "CRITICAL:root:Average generation speed: 10.81 tokens/s. Total generated tokens: 306.\n",
            "CRITICAL:root:Average generation speed: 10.89 tokens/s. Total generated tokens: 318.\n",
            "CRITICAL:root:Average generation speed: 10.91 tokens/s. Total generated tokens: 330.\n",
            "CRITICAL:root:Average generation speed: 10.98 tokens/s. Total generated tokens: 341.\n",
            "CRITICAL:root:Average generation speed: 11.04 tokens/s. Total generated tokens: 349.\n",
            "CRITICAL:root:Average generation speed: 11.00 tokens/s. Total generated tokens: 361.\n",
            "CRITICAL:root:Average generation speed: 10.93 tokens/s. Total generated tokens: 368.\n",
            "CRITICAL:root:Average generation speed: 10.81 tokens/s. Total generated tokens: 382.\n",
            "CRITICAL:root:Average generation speed: 10.72 tokens/s. Total generated tokens: 388.\n",
            "CRITICAL:root:Average generation speed: 10.69 tokens/s. Total generated tokens: 402.\n",
            "CRITICAL:root:Average generation speed: 10.67 tokens/s. Total generated tokens: 412.\n",
            "CRITICAL:root:Average generation speed: 10.66 tokens/s. Total generated tokens: 422.\n",
            "CRITICAL:root:Average generation speed: 10.69 tokens/s. Total generated tokens: 433.\n",
            "CRITICAL:root:Average generation speed: 10.57 tokens/s. Total generated tokens: 444.\n",
            "CRITICAL:root:Average generation speed: 10.60 tokens/s. Total generated tokens: 456.\n",
            "CRITICAL:root:Average generation speed: 10.66 tokens/s. Total generated tokens: 467.\n",
            "CRITICAL:root:Average generation speed: 10.67 tokens/s. Total generated tokens: 479.\n",
            "CRITICAL:root:Average generation speed: 10.66 tokens/s. Total generated tokens: 488.\n",
            "CRITICAL:root:Average generation speed: 10.59 tokens/s. Total generated tokens: 496.\n",
            "CRITICAL:root:Average generation speed: 10.52 tokens/s. Total generated tokens: 500.\n",
            "CRITICAL:root:Average generation speed: 10.49 tokens/s. Total generated tokens: 517.\n",
            "CRITICAL:root:Average generation speed: 10.35 tokens/s. Total generated tokens: 529.\n",
            "CRITICAL:root:Average generation speed: 10.36 tokens/s. Total generated tokens: 537.\n",
            "CRITICAL:root:Average generation speed: 10.40 tokens/s. Total generated tokens: 551.\n",
            "CRITICAL:root:Average generation speed: 10.43 tokens/s. Total generated tokens: 562.\n",
            "CRITICAL:root:Average generation speed: 10.41 tokens/s. Total generated tokens: 572.\n",
            "CRITICAL:root:Average generation speed: 10.41 tokens/s. Total generated tokens: 583.\n",
            "CRITICAL:root:Average generation speed: 10.43 tokens/s. Total generated tokens: 594.\n",
            "CRITICAL:root:Average generation speed: 10.43 tokens/s. Total generated tokens: 604.\n",
            "CRITICAL:root:Average generation speed: 10.33 tokens/s. Total generated tokens: 617.\n",
            "CRITICAL:root:Average generation speed: 10.33 tokens/s. Total generated tokens: 627.\n",
            "CRITICAL:root:Average generation speed: 10.27 tokens/s. Total generated tokens: 640.\n",
            "CRITICAL:root:Average generation speed: 10.27 tokens/s. Total generated tokens: 651.\n",
            "CRITICAL:root:Average generation speed: 10.27 tokens/s. Total generated tokens: 661.\n",
            "CRITICAL:root:Average generation speed: 10.25 tokens/s. Total generated tokens: 673.\n",
            "CRITICAL:root:Average generation speed: 10.25 tokens/s. Total generated tokens: 685.\n",
            "CRITICAL:root:Inference time is 1710089888.1860738 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4113ef58752bd4babd.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INT 4 - compressed model"
      ],
      "metadata": {
        "id": "R9WiuiS81WUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and convert the model into OV IR format.**\n",
        "\n",
        "The weight format is set to INT4. We are using Optimum Intel CLI to get the compressed and converted model from Huggingface.\n"
      ],
      "metadata": {
        "id": "xw4mm3wa2xYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!optimum-cli export openvino --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format int4 --ratio 0.9 --group-size 128 ov_model_cli_int4_tinyllama"
      ],
      "metadata": {
        "id": "wbhxBLUdyS8s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "48f1c2df-9ea9-481c-ddeb-4b4eaffcf05a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-10 20:55:23.882117: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-10 20:55:23.882178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-10 20:55:23.893918: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-10 20:55:30.262312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "Framework not specified. Using pt to export the model.\n",
            "Automatic task detection to text-generation-with-past (possible synonyms are: causal-lm-with-past).\n",
            "Using the export variant default. Available variants are:\n",
            "    - default: The default ONNX variant.\n",
            "Using framework PyTorch: 2.1.0+cu121\n",
            "WARNING:root:\u001b[1;31m[WARNING] For good performance with stateful models, transformers>=4.36.2 and PyTorch>=2.1.1 are required. This Python environment has Transformers 4.38.2 and PyTorch 2.1.0+cu121. Consider upgrading PyTorch and Transformers, for example by running `pip install --upgrade --upgrade-strategy eager optimum[openvino]`, and export the model again\u001b[0m\n",
            "WARNING:root:Cannot apply model.to_bettertransformer because of the exception:\n",
            "Transformers now supports natively BetterTransformer optimizations (torch.nn.functional.scaled_dot_product_attention) for the model type llama. As such, there is no need to use `model.to_bettertransformers()` or `BetterTransformer.transform(model)` from the Optimum library. Please upgrade to transformers>=4.36 and torch>=2.1.1 to use it. Details: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention.. Usage model with stateful=True may be non-effective if model does not contain torch.functional.scaled_dot_product_attention\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> True\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
            "The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:1068: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if seq_length > self.causal_mask.shape[-1]:\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py:385: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
            "\u001b[2KMixed-Precision assignment \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m154/154\u001b[0m • \u001b[36m0:00:15\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO:nncf:Statistics of the bitwidth distribution:\n",
            "+--------------+---------------------------+-----------------------------------+\n",
            "| Num bits (N) | % all parameters (layers) |    % ratio-defining parameters    |\n",
            "|              |                           |             (layers)              |\n",
            "+==============+===========================+===================================+\n",
            "| 8            | 21% (32 / 156)            | 11% (30 / 154)                    |\n",
            "+--------------+---------------------------+-----------------------------------+\n",
            "| 4            | 79% (124 / 156)           | 89% (124 / 154)                   |\n",
            "+--------------+---------------------------+-----------------------------------+\n",
            "\u001b[2KApplying Weight Compression \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m156/156\u001b[0m • \u001b[36m0:00:48\u001b[0m • \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the size of the INT4 compressed model**"
      ],
      "metadata": {
        "id": "PxPjiSm522Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "int4_model_dir = Path(\"/content/ov_model_cli_int4_tinyllama\")\n",
        "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "\n",
        "if int4_weights.exists():\n",
        "    print(\n",
        "        f\"Size of model with INT4 compressed weights is {int4_weights.stat().st_size / 1024 / 1024:.2f} MB\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7UYt01CLymj7",
        "outputId": "28ba2431-fd40-40b9-e93e-a0bbfd5e4500"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of model with INT4 compressed weights is 670.40 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create an OV object for the compressed model**"
      ],
      "metadata": {
        "id": "REjA-Ng528DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.intel.openvino import OVModelForCausalLM, OVWeightQuantizationConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_dir = int4_model_dir\n",
        "\n",
        "print(f\"Loading model from {model_dir}\")\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device=\"CPU\",\n",
        "    ov_config=ov_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SJK4Jo-3h1Pg",
        "outputId": "7f8df076-b21c-43be-8968-c39794e1f16b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/ov_model_cli_int4_tinyllama\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Compiling the model to CPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions for Generation**"
      ],
      "metadata": {
        "id": "B_6aJhdW3Bad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from time import perf_counter\n",
        "from typing import List\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, TextIteratorStreamer\n",
        "import numpy as np\n",
        "\n",
        "model_configuration = {\n",
        "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"prompt_template\":  \"<|user|>\\n{instruction}</s> \\n<|assistant|>\\n\",\n",
        "        \"tokenizer_kwargs\": {\"add_special_tokens\": False},\n",
        "    }\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
        "\n",
        "\n",
        "def get_special_token_id(tokenizer: AutoTokenizer, key: str) -> int:\n",
        "    \"\"\"\n",
        "    Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
        "        key (str): the key to convert to a single token\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: if more than one ID was generated\n",
        "\n",
        "    Returns:\n",
        "        int: the token ID for the given key\n",
        "    \"\"\"\n",
        "    token_ids = tokenizer.encode(key)\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
        "    return token_ids[0]\n",
        "\n",
        "response_key = model_configuration.get(\"response_key\")\n",
        "tokenizer_response_key = None\n",
        "\n",
        "if response_key is not None:\n",
        "    tokenizer_response_key = next((token for token in tokenizer.additional_special_tokens if token.startswith(response_key)), None)\n",
        "\n",
        "end_key_token_id = None\n",
        "if tokenizer_response_key:\n",
        "    try:\n",
        "        end_key = model_configuration.get(\"end_key\")\n",
        "        if end_key:\n",
        "            end_key_token_id = get_special_token_id(tokenizer, end_key)\n",
        "        # Ensure generation stops once it generates \"### End\"\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "prompt_template = model_configuration.get(\"prompt_template\", \"{instruction}\")\n",
        "end_key_token_id = end_key_token_id or tokenizer.eos_token_id\n",
        "pad_token_id = end_key_token_id or tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "_caljrKuymp7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def estimate_latency(current_time:float, current_perf_text:str, new_gen_text:str, per_token_time:List[float], num_tokens:int):\n",
        "    \"\"\"\n",
        "    Helper function for performance estimation\n",
        "\n",
        "    Parameters:\n",
        "      current_time (float): This step time in seconds.\n",
        "      current_perf_text (str): Current content of performance UI field.\n",
        "      new_gen_text (str): New generated text.\n",
        "      per_token_time (List[float]): history of performance from previous steps.\n",
        "      num_tokens (int): Total number of generated tokens.\n",
        "\n",
        "    Returns:\n",
        "      update for performance text field\n",
        "      update for a total number of tokens\n",
        "    \"\"\"\n",
        "    # start = time.time()\n",
        "    num_current_toks = len(tokenizer.encode(new_gen_text))\n",
        "    num_tokens += num_current_toks\n",
        "    per_token_time.append(num_current_toks / current_time)\n",
        "    if len(per_token_time) > 10 and len(per_token_time) % 4 == 0:\n",
        "        current_bucket = per_token_time[:-10]\n",
        "        # end = time.time()\n",
        "        logging.critical(f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}.\")\n",
        "        return f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}.\", num_tokens\n",
        "    # current_perf_text = f\"Average generation speed: {np.mean(per_token_time):.2f} tokens/s. Total generated tokens: {num_tokens}\"\n",
        "    return current_perf_text, num_tokens"
      ],
      "metadata": {
        "id": "wUTKCX4jFssB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation Function**"
      ],
      "metadata": {
        "id": "v67jgtmR3Pk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "import logging\n",
        "import time\n",
        "def run_generation(user_text:str, chat_history:str):\n",
        "    \"\"\"\n",
        "    Text generation function\n",
        "\n",
        "    Parameters:\n",
        "      user_text (str): User-provided instruction for a generation.\n",
        "      top_p (float):  Nucleus sampling. If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for a generation.\n",
        "      temperature (float): The value used to module the logits distribution.\n",
        "      top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "      max_new_tokens (int): Maximum length of generated sequence.\n",
        "      perf_text (str): Content of text field for printing performance results.\n",
        "    Returns:\n",
        "      model_output (str) - model-generated text\n",
        "      perf_text (str) - updated perf text filed content\n",
        "    \"\"\"\n",
        "    # if user_text == \"Hello\" or user_text == \"hello\":\n",
        "    #   return \"Hello!! Nice to see you here. I can follow instructions and generate text for you.\", \"Will show average generation speed and number of tokens generated here.\"\n",
        "\n",
        "    # Prepare input prompt according to model expected template\n",
        "    prompt_text = prompt_template.format(instruction=user_text)\n",
        "\n",
        "    # Tokenize the user text.\n",
        "    model_inputs = tokenizer(prompt_text, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "\n",
        "    # Start generation on a separate thread, so that we don't block the UI. The text is pulled from the streamer\n",
        "    # in the main thread. Adds timeout to the streamer to handle exceptions in the generation thread.\n",
        "\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        top_p=0.92,\n",
        "        temperature=float(0.8),\n",
        "        top_k=0,\n",
        "        eos_token_id=end_key_token_id,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "    start = time.time()\n",
        "    t = Thread(target=ov_model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "    end = time.time()\n",
        "\n",
        "\n",
        "\n",
        "    # Pull the generated text from the streamer, and update the model output.\n",
        "    model_output = \"\"\n",
        "    per_token_time = []\n",
        "    num_tokens = 0\n",
        "    start = perf_counter()\n",
        "\n",
        "    for new_text in streamer:\n",
        "        current_time = perf_counter() - start\n",
        "        model_output += new_text\n",
        "        perf_text, num_tokens = estimate_latency(current_time, \"\", new_text, per_token_time, num_tokens)\n",
        "        yield model_output\n",
        "        start = perf_counter()\n",
        "    logging.critical(f\"Inference time is {end-start} seconds\")\n",
        "    chat_history.append((user_text, model_output))\n",
        "    return model_output, chat_history"
      ],
      "metadata": {
        "id": "-0VOhCCiKOfy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference Results**"
      ],
      "metadata": {
        "id": "ML-KEb973YSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm"
      ],
      "metadata": {
        "id": "bx9nE6CrFHYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  gr.ChatInterface(run_generation)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue()\n",
        "    try:\n",
        "        demo.launch(debug = True)\n",
        "    except Exception:\n",
        "        demo.launch(share=True, debug = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1273
        },
        "id": "2muEXFexJPGu",
        "outputId": "fa2131c3-bdda-4686-cf3f-cb8e49c77bc2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0e9eb7508749feffe6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e9eb7508749feffe6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CRITICAL:root:Average generation speed: 2.42 tokens/s. Total generated tokens: 32.\n",
            "CRITICAL:root:Average generation speed: 8.35 tokens/s. Total generated tokens: 36.\n",
            "CRITICAL:root:Average generation speed: 10.34 tokens/s. Total generated tokens: 51.\n",
            "CRITICAL:root:Average generation speed: 9.76 tokens/s. Total generated tokens: 60.\n",
            "CRITICAL:root:Average generation speed: 10.08 tokens/s. Total generated tokens: 72.\n",
            "CRITICAL:root:Average generation speed: 9.90 tokens/s. Total generated tokens: 83.\n",
            "CRITICAL:root:Average generation speed: 9.27 tokens/s. Total generated tokens: 95.\n",
            "CRITICAL:root:Average generation speed: 9.17 tokens/s. Total generated tokens: 106.\n",
            "CRITICAL:root:Average generation speed: 9.38 tokens/s. Total generated tokens: 116.\n",
            "CRITICAL:root:Average generation speed: 9.43 tokens/s. Total generated tokens: 128.\n",
            "CRITICAL:root:Average generation speed: 9.78 tokens/s. Total generated tokens: 138.\n",
            "CRITICAL:root:Average generation speed: 9.69 tokens/s. Total generated tokens: 145.\n",
            "CRITICAL:root:Average generation speed: 9.95 tokens/s. Total generated tokens: 159.\n",
            "CRITICAL:root:Average generation speed: 9.86 tokens/s. Total generated tokens: 171.\n",
            "CRITICAL:root:Average generation speed: 9.94 tokens/s. Total generated tokens: 183.\n",
            "CRITICAL:root:Average generation speed: 10.00 tokens/s. Total generated tokens: 187.\n",
            "CRITICAL:root:Average generation speed: 10.10 tokens/s. Total generated tokens: 202.\n",
            "CRITICAL:root:Average generation speed: 9.94 tokens/s. Total generated tokens: 213.\n",
            "CRITICAL:root:Average generation speed: 9.78 tokens/s. Total generated tokens: 225.\n",
            "CRITICAL:root:Average generation speed: 9.63 tokens/s. Total generated tokens: 236.\n",
            "CRITICAL:root:Average generation speed: 9.63 tokens/s. Total generated tokens: 247.\n",
            "CRITICAL:root:Average generation speed: 9.72 tokens/s. Total generated tokens: 257.\n",
            "CRITICAL:root:Average generation speed: 9.76 tokens/s. Total generated tokens: 267.\n",
            "CRITICAL:root:Average generation speed: 9.71 tokens/s. Total generated tokens: 276.\n",
            "CRITICAL:root:Average generation speed: 9.74 tokens/s. Total generated tokens: 283.\n",
            "CRITICAL:root:Average generation speed: 9.74 tokens/s. Total generated tokens: 296.\n",
            "CRITICAL:root:Average generation speed: 9.70 tokens/s. Total generated tokens: 306.\n",
            "CRITICAL:root:Average generation speed: 9.69 tokens/s. Total generated tokens: 319.\n",
            "CRITICAL:root:Average generation speed: 9.71 tokens/s. Total generated tokens: 329.\n",
            "CRITICAL:root:Average generation speed: 9.71 tokens/s. Total generated tokens: 341.\n",
            "CRITICAL:root:Average generation speed: 9.63 tokens/s. Total generated tokens: 352.\n",
            "CRITICAL:root:Average generation speed: 9.50 tokens/s. Total generated tokens: 362.\n",
            "CRITICAL:root:Inference time is 1710089845.9038959 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://0e9eb7508749feffe6.gradio.live\n"
          ]
        }
      ]
    }
  ]
}