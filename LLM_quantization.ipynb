{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPB4x4tagSBv",
        "outputId": "e1226f00-fad8-4e3b-a972-72b03c6fa7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q openvino-nightly \"nncf>=2.7\" \"transformers>=4.36.0\" onnx \"optimum>=1.16.1\" \"accelerate\" \"datasets\" gradio \"git+https://github.com/huggingface/optimum-intel.git\" --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI88leBhVl8o",
        "outputId": "0caed400-f3b2-4df4-dd02-960d605dd378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CPU:  2\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from psutil import *\n",
        "# This code will return the number of CPU\n",
        "print(\"Number of CPU: \", cpu_count())\n",
        "# This code will return the CPU info\n",
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcvmY9bCxzSb",
        "outputId": "ee1e0ee3-b77c-45c4-85c4-1d16d430cf7a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:          6\n",
            "    Model:               79\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4399.99\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsa\n",
            "                         veopt arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    256 KiB (1 instance)\n",
            "  L3:                    55 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XgLSfu2Qhoit"
      },
      "outputs": [],
      "source": [
        "SUPPORTED_LLM_MODELS = {\n",
        "    \"tiny-llama-1b\": {\n",
        "        \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"prompt_template\":  \"<|user|>\\n{instruction}</s> \\n<|assistant|>\\n\",\n",
        "        \"tokenizer_kwargs\": {\"add_special_tokens\": False},\n",
        "    },\n",
        "    \"phi-2\": {\n",
        "        \"model_id\": \"susnato/phi-2\",\n",
        "        \"prompt_template\": \"Instruct:{instruction}\\nOutput:\"\n",
        "    },\n",
        "    \"dolly-v2-3b\": {\n",
        "        \"model_id\": \"databricks/dolly-v2-3b\",\n",
        "        \"instriction_key\": \"### Instruction:\",\n",
        "        \"response_key\": \"### Response:\",\n",
        "        \"end_key\": \"### End\",\n",
        "        \"prompt_template\": \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    },\n",
        "    \"red-pajama-instruct-3b\": {\n",
        "        \"model_id\": \"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\n",
        "        \"prompt_template\": \"Q: {instruction}\\nA:\"\n",
        "    },\n",
        "    \"mistral-7b\": {\n",
        "        \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "        \"prompt_template\": \"<s> [INST] {instruction} [/INST] </s>\",\n",
        "        \"tokenizer_kwargs\": {\"add_special_tokens\": False},\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mhnhmTJnhgvv"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "e7b4f151c3e641a6aa1064b1349b0763",
            "2fd5332e05594c7d91760e7dfe73b104",
            "9d65f92673d94ce8b536f096599c936d"
          ]
        },
        "id": "aUUjy8gDhutv",
        "outputId": "d499e5b2-4db6-4def-ac92-78ccba115712"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RadioButtons(description='Model:', index=1, options=('tiny-llama-1b', 'phi-2', 'dolly-v2-3b', 'red-pajama-inst…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7b4f151c3e641a6aa1064b1349b0763"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_ids = list(SUPPORTED_LLM_MODELS)\n",
        "\n",
        "# model_id = widgets.Dropdown(\n",
        "#     options=model_ids,\n",
        "#     value=model_ids[1],\n",
        "#     description=\"Model:\",\n",
        "#     disabled=False,\n",
        "# )\n",
        "\n",
        "# model_id\n",
        "# widgets.GridBox()\n",
        "\n",
        "model_id = widgets.RadioButtons(\n",
        "    options=model_ids,\n",
        "    value=model_ids[1],\n",
        "    description=\"Model:\",\n",
        "    disabled=False,\n",
        ")\n",
        "model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2W5UFIdixMc",
        "outputId": "5b85b660-a17d-405e-ff99-eb4502fa3b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model tiny-llama-1b\n",
            "{'model_id': 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', 'prompt_template': '<|user|>\\n{instruction}</s> \\n<|assistant|>\\n', 'tokenizer_kwargs': {'add_special_tokens': False}}\n"
          ]
        }
      ],
      "source": [
        "model_configuration = SUPPORTED_LLM_MODELS[model_id.value]\n",
        "print(f\"Selected model {model_id.value}\")\n",
        "print(model_configuration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109,
          "referenced_widgets": [
            "870a32ce89ea4dd9ad703c9ebc051946",
            "0397e2ecf1a44e21835775e643498135",
            "f5d52d38f56441fa976e6c791c591596",
            "c8f33069e1f247c08310a57f8116a95c",
            "50dbbe13a4464ee780c9c7b50146339b",
            "58e6b3e2bd034286a133a2e87f136314",
            "2efcfda2ad444638825e9b5f3a36e03a",
            "6965aaab0c8845b48ab3864c4523a864",
            "69765c609f324c25bea5046a446db9e8"
          ]
        },
        "id": "_13MDAyfjIFm",
        "outputId": "252f1b59-0ea9-487f-de82-71e798e97261"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=True, description='Prepare INT4 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "870a32ce89ea4dd9ad703c9ebc051946"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Prepare INT8 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8f33069e1f247c08310a57f8116a95c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Prepare FP16 model')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2efcfda2ad444638825e9b5f3a36e03a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "prepare_int4_model = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Prepare INT4 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "prepare_int8_model = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"Prepare INT8 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "prepare_fp16_model = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description=\"Prepare FP16 model\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "display(prepare_int4_model)\n",
        "display(prepare_int8_model)\n",
        "display(prepare_fp16_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff0OZR0Tje7T",
        "outputId": "1f5e58a4-2882-4756-cee9-9771fb33d417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import logging\n",
        "import openvino as ov\n",
        "import nncf\n",
        "from optimum.intel.openvino import OVModelForCausalLM, OVWeightQuantizationConfig\n",
        "from optimum.utils import NormalizedTextConfig, NormalizedConfigManager\n",
        "import gc\n",
        "\n",
        "NormalizedConfigManager._conf['phi'] = NormalizedTextConfig\n",
        "\n",
        "nncf.set_log_level(logging.ERROR)\n",
        "\n",
        "pt_model_id = model_configuration[\"model_id\"]\n",
        "print(pt_model_id)\n",
        "fp16_model_dir = Path(model_id.value) / \"FP16\"\n",
        "int8_model_dir = Path(model_id.value) / \"INT8_compressed_weights\"\n",
        "int4_model_dir = Path(model_id.value) / \"INT4_compressed_weights\"\n",
        "\n",
        "core = ov.Core()\n",
        "\n",
        "def convert_to_fp16():\n",
        "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    ov_model = OVModelForCausalLM.from_pretrained(pt_model_id, export=True, compile=False, load_in_8bit=False)\n",
        "    ov_model.half()\n",
        "    ov_model.save_pretrained(fp16_model_dir)\n",
        "    del ov_model\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def convert_to_int8():\n",
        "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    ov_model = OVModelForCausalLM.from_pretrained(pt_model_id, export=True, compile=False, load_in_8bit=True)\n",
        "    ov_model.save_pretrained(int8_model_dir)\n",
        "    del ov_model\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def convert_to_int4():\n",
        "    compression_configs = {\n",
        "        \"mistral-7b\": {\n",
        "            \"sym\": True,\n",
        "            \"group_size\": 64,\n",
        "            \"ratio\": 0.6,\n",
        "        },\n",
        "        'red-pajama-3b-instruct': {\n",
        "            \"sym\": False,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.5,\n",
        "        },\n",
        "        \"dolly-v2-3b\": {\n",
        "            \"sym\": False,\n",
        "            \"group_size\": 32,\n",
        "            \"ratio\": 0.5\n",
        "        },\n",
        "        \"default\": {\n",
        "            \"sym\": False,\n",
        "            \"group_size\": 128,\n",
        "            \"ratio\": 0.8,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    model_compression_params = compression_configs.get(\n",
        "        model_id.value, compression_configs[\"default\"]\n",
        "    )\n",
        "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
        "        return\n",
        "    ov_model = OVModelForCausalLM.from_pretrained(\n",
        "        pt_model_id, export=True, compile=False,\n",
        "        quantization_config=OVWeightQuantizationConfig(bits=4, **model_compression_params)\n",
        "    )\n",
        "    ov_model.save_pretrained(int4_model_dir)\n",
        "    del ov_model\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "if prepare_fp16_model.value:\n",
        "    convert_to_fp16()\n",
        "if prepare_int8_model.value:\n",
        "    convert_to_int8()\n",
        "if prepare_int4_model.value:\n",
        "    convert_to_int4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Iqe-GEplHmo",
        "outputId": "ef7f2b53-5885-4e65-fceb-441330bdd998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of model with INT8 compressed weights is 1055.54 MB\n"
          ]
        }
      ],
      "source": [
        "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
        "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
        "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
        "\n",
        "if fp16_weights.exists():\n",
        "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
        "    if compressed_weights.exists():\n",
        "        print(\n",
        "            f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\"\n",
        "        )\n",
        "    if compressed_weights.exists() and fp16_weights.exists():\n",
        "        print(\n",
        "            f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f865af6676a54c8ca51cb51f318f85d6",
            "a5dd9b3899154607994fceab27d65f21",
            "9cb470776e0443e39b5b40b1fd893b36"
          ]
        },
        "id": "QB3mjzYmlMZn",
        "outputId": "2222c373-6ea3-4e1f-8b70-a059be92df1b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RadioButtons(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f865af6676a54c8ca51cb51f318f85d6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "core = ov.Core()\n",
        "device = widgets.RadioButtons(\n",
        "    options=core.available_devices + [\"AUTO\"],\n",
        "    value=\"CPU\",\n",
        "    description=\"Device:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8634ab4037fd4f04a43ef5ad2f19ecbc",
            "28deaea0466d451bb3216c7974ff40f8",
            "0063decb3d3b4d9d8e8f2a561711acff"
          ]
        },
        "id": "lRwb5SAOlYMn",
        "outputId": "17c140d3-ac06-4943-9a48-ad41d7283490"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "RadioButtons(description='Model to run:', options=('INT8',), value='INT8')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8634ab4037fd4f04a43ef5ad2f19ecbc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "available_models = []\n",
        "if int4_model_dir.exists():\n",
        "    available_models.append(\"INT4\")\n",
        "if int8_model_dir.exists():\n",
        "    available_models.append(\"INT8\")\n",
        "if fp16_model_dir.exists():\n",
        "    available_models.append(\"FP16\")\n",
        "\n",
        "model_to_run = widgets.RadioButtons(\n",
        "    options=available_models,\n",
        "    value=available_models[0],\n",
        "    description=\"Model to run:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_to_run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po8edtcGlhFl",
        "outputId": "3152b832-a82c-479b-b3e3-ad155e4b8863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from tiny-llama-1b/INT8_compressed_weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Compiling the model to CPU ...\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "if model_to_run.value == \"INT4\":\n",
        "    model_dir = int4_model_dir\n",
        "elif model_to_run.value == \"INT8\":\n",
        "    model_dir = int8_model_dir\n",
        "else:\n",
        "    model_dir = fp16_model_dir\n",
        "print(f\"Loading model from {model_dir}\")\n",
        "\n",
        "model_name = model_configuration[\"model_id\"]\n",
        "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "ov_model = OVModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device=device.value,\n",
        "    ov_config=ov_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPB6d47clmWl"
      },
      "source": [
        "#Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "5-Id5LsBlo7L"
      },
      "outputs": [],
      "source": [
        "from threading import Thread\n",
        "from time import perf_counter\n",
        "from typing import List\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, TextIteratorStreamer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ubn4tVDnl7hB"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_kwargs = model_configuration.get(\"toeknizer_kwargs\", {})\n",
        "\n",
        "\n",
        "def get_special_token_id(tokenizer: AutoTokenizer, key: str) -> int:\n",
        "    \"\"\"\n",
        "    Gets the token ID for a given string that has been added to the tokenizer as a special token.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): the tokenizer\n",
        "        key (str): the key to convert to a single token\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: if more than one ID was generated\n",
        "\n",
        "    Returns:\n",
        "        int: the token ID for the given key\n",
        "    \"\"\"\n",
        "    token_ids = tokenizer.encode(key)\n",
        "    if len(token_ids) > 1:\n",
        "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
        "    return token_ids[0]\n",
        "\n",
        "response_key = model_configuration.get(\"response_key\")\n",
        "tokenizer_response_key = None\n",
        "\n",
        "if response_key is not None:\n",
        "    tokenizer_response_key = next((token for token in tokenizer.additional_special_tokens if token.startswith(response_key)), None)\n",
        "\n",
        "end_key_token_id = None\n",
        "if tokenizer_response_key:\n",
        "    try:\n",
        "        end_key = model_configuration.get(\"end_key\")\n",
        "        if end_key:\n",
        "            end_key_token_id = get_special_token_id(tokenizer, end_key)\n",
        "        # Ensure generation stops once it generates \"### End\"\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "prompt_template = model_configuration.get(\"prompt_template\", \"{instruction}\")\n",
        "end_key_token_id = end_key_token_id or tokenizer.eos_token_id\n",
        "pad_token_id = end_key_token_id or tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YVZoiM2zmHUw"
      },
      "outputs": [],
      "source": [
        "def run_generation(user_text:str, top_p:float, temperature:float, top_k:int, max_new_tokens:int, perf_text:str):\n",
        "    \"\"\"\n",
        "    Text generation function\n",
        "\n",
        "    Parameters:\n",
        "      user_text (str): User-provided instruction for a generation.\n",
        "      top_p (float):  Nucleus sampling. If set to < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for a generation.\n",
        "      temperature (float): The value used to module the logits distribution.\n",
        "      top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "      max_new_tokens (int): Maximum length of generated sequence.\n",
        "      perf_text (str): Content of text field for printing performance results.\n",
        "    Returns:\n",
        "      model_output (str) - model-generated text\n",
        "      perf_text (str) - updated perf text filed content\n",
        "    \"\"\"\n",
        "\n",
        "    # Prepare input prompt according to model expected template\n",
        "    prompt_text = prompt_template.format(instruction=user_text)\n",
        "\n",
        "    # Tokenize the user text.\n",
        "    model_inputs = tokenizer(prompt_text, return_tensors=\"pt\", **tokenizer_kwargs)\n",
        "\n",
        "    # Start generation on a separate thread, so that we don't block the UI. The text is pulled from the streamer\n",
        "    # in the main thread. Adds timeout to the streamer to handle exceptions in the generation thread.\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    generate_kwargs = dict(\n",
        "        model_inputs,\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        temperature=float(temperature),\n",
        "        top_k=top_k,\n",
        "        eos_token_id=end_key_token_id,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "    t = Thread(target=ov_model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    # Pull the generated text from the streamer, and update the model output.\n",
        "    model_output = \"\"\n",
        "    per_token_time = []\n",
        "    num_tokens = 0\n",
        "    start = perf_counter()\n",
        "    for new_text in streamer:\n",
        "        current_time = perf_counter() - start\n",
        "        model_output += new_text\n",
        "        perf_text, num_tokens = estimate_latency(current_time, perf_text, new_text, per_token_time, num_tokens)\n",
        "        yield model_output, perf_text\n",
        "        start = perf_counter()\n",
        "    return model_output, perf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "19ZjsCH_mnHO"
      },
      "outputs": [],
      "source": [
        "def estimate_latency(current_time:float, current_perf_text:str, new_gen_text:str, per_token_time:List[float], num_tokens:int):\n",
        "    \"\"\"\n",
        "    Helper function for performance estimation\n",
        "\n",
        "    Parameters:\n",
        "      current_time (float): This step time in seconds.\n",
        "      current_perf_text (str): Current content of performance UI field.\n",
        "      new_gen_text (str): New generated text.\n",
        "      per_token_time (List[float]): history of performance from previous steps.\n",
        "      num_tokens (int): Total number of generated tokens.\n",
        "\n",
        "    Returns:\n",
        "      update for performance text field\n",
        "      update for a total number of tokens\n",
        "    \"\"\"\n",
        "    num_current_toks = len(tokenizer.encode(new_gen_text))\n",
        "    num_tokens += num_current_toks\n",
        "    per_token_time.append(num_current_toks / current_time)\n",
        "    if len(per_token_time) > 10 and len(per_token_time) % 4 == 0:\n",
        "        current_bucket = per_token_time[:-10]\n",
        "        return f\"Average generation speed: {np.mean(current_bucket):.2f} tokens/s. Total generated tokens: {num_tokens}\", num_tokens\n",
        "    return current_perf_text, num_tokens\n",
        "\n",
        "def reset_textbox(instruction:str, response:str, perf:str):\n",
        "    \"\"\"\n",
        "    Helper function for resetting content of all text fields\n",
        "\n",
        "    Parameters:\n",
        "      instruction (str): Content of user instruction field.\n",
        "      response (str): Content of model response field.\n",
        "      perf (str): Content of performance info filed\n",
        "\n",
        "    Returns:\n",
        "      empty string for each placeholder\n",
        "    \"\"\"\n",
        "    return \"\", \"\", \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "JjNrHmxzmtRN",
        "outputId": "2b1c022b-f031-4150-e25b-47a4038225d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://175eddb1423ec7697f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://175eddb1423ec7697f.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "examples = [\n",
        "    \"Give me a recipe for pizza with pineapple\",\n",
        "    \"Write me a tweet about the new OpenVINO release\",\n",
        "    \"Explain the difference between CPU and GPU\",\n",
        "    \"Give five ideas for a great weekend with family\",\n",
        "    \"Do Androids dream of Electric sheep?\",\n",
        "    \"Who is Dolly?\",\n",
        "    \"Please give me advice on how to write resume?\",\n",
        "    \"Name 3 advantages to being a cat\",\n",
        "    \"Write instructions on how to become a good AI engineer\",\n",
        "    \"Write a love letter to my best friend\",\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "        \"# Question Answering with \" + model_id.value + \" and OpenVINO.\\n\"\n",
        "        \"Provide instruction which describes a task below or select among predefined examples and model writes response that performs requested task.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            user_text = gr.Textbox(\n",
        "                placeholder=\"Write an email about an alpaca that likes flan\",\n",
        "                label=\"User instruction\"\n",
        "            )\n",
        "            model_output = gr.Textbox(label=\"Model response\", interactive=False)\n",
        "            performance = gr.Textbox(label=\"Performance\", lines=1, interactive=False)\n",
        "            with gr.Column(scale=1):\n",
        "                button_clear = gr.Button(value=\"Clear\")\n",
        "                button_submit = gr.Button(value=\"Submit\")\n",
        "            gr.Examples(examples, user_text)\n",
        "        with gr.Column(scale=1):\n",
        "            max_new_tokens = gr.Slider(\n",
        "                minimum=1, maximum=1000, value=256, step=1, interactive=True, label=\"Max New Tokens\",\n",
        "            )\n",
        "            top_p = gr.Slider(\n",
        "                minimum=0.05, maximum=1.0, value=0.92, step=0.05, interactive=True, label=\"Top-p (nucleus sampling)\",\n",
        "            )\n",
        "            top_k = gr.Slider(\n",
        "                minimum=0, maximum=50, value=0, step=1, interactive=True, label=\"Top-k\",\n",
        "            )\n",
        "            temperature = gr.Slider(\n",
        "                minimum=0.1, maximum=5.0, value=0.8, step=0.1, interactive=True, label=\"Temperature\",\n",
        "            )\n",
        "\n",
        "    user_text.submit(run_generation, [user_text, top_p, temperature, top_k, max_new_tokens, performance], [model_output, performance])\n",
        "    button_submit.click(run_generation, [user_text, top_p, temperature, top_k, max_new_tokens, performance], [model_output, performance])\n",
        "    button_clear.click(reset_textbox, [user_text, model_output, performance], [user_text, model_output, performance])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue()\n",
        "    try:\n",
        "        demo.launch(height=800)\n",
        "    except Exception:\n",
        "        demo.launch(share=True, height=800)\n",
        "\n",
        "# If you are launching remotely, specify server_name and server_port\n",
        "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
        "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyQTIA5knzIm"
      },
      "source": [
        "#Using Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqcSOVyrn-W1",
        "outputId": "12547be6-71f1-4e99-c316-0ef7a5a59854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Oh7OWOzcN9",
        "outputId": "38f5be38-933a-41ce-d0af-f47d7bd5516e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors in 5.102s\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppw6CyG57KFH",
        "outputId": "403ab9f8-aba5-4675-f0cf-d69d887b90de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35.189.174.69\n"
          ]
        }
      ],
      "source": [
        "!curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqIzZFStziEt"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app_without_perf_numbers.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAMJisw_zlJl",
        "outputId": "ce94af1d-49e5-49cd-8397-4f5a48655a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.563s\n",
            "your url is: https://tangy-sheep-yell.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7b4f151c3e641a6aa1064b1349b0763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "RadioButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "RadioButtonsModel",
            "_options_labels": [
              "tiny-llama-1b",
              "phi-2",
              "dolly-v2-3b",
              "red-pajama-instruct-3b",
              "mistral-7b"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "RadioButtonsView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_2fd5332e05594c7d91760e7dfe73b104",
            "style": "IPY_MODEL_9d65f92673d94ce8b536f096599c936d"
          }
        },
        "2fd5332e05594c7d91760e7dfe73b104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d65f92673d94ce8b536f096599c936d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "870a32ce89ea4dd9ad703c9ebc051946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare INT4 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0397e2ecf1a44e21835775e643498135",
            "style": "IPY_MODEL_f5d52d38f56441fa976e6c791c591596",
            "value": false
          }
        },
        "0397e2ecf1a44e21835775e643498135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d52d38f56441fa976e6c791c591596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f33069e1f247c08310a57f8116a95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare INT8 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_50dbbe13a4464ee780c9c7b50146339b",
            "style": "IPY_MODEL_58e6b3e2bd034286a133a2e87f136314",
            "value": true
          }
        },
        "50dbbe13a4464ee780c9c7b50146339b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e6b3e2bd034286a133a2e87f136314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2efcfda2ad444638825e9b5f3a36e03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Prepare FP16 model",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6965aaab0c8845b48ab3864c4523a864",
            "style": "IPY_MODEL_69765c609f324c25bea5046a446db9e8",
            "value": false
          }
        },
        "6965aaab0c8845b48ab3864c4523a864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69765c609f324c25bea5046a446db9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f865af6676a54c8ca51cb51f318f85d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "RadioButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "RadioButtonsModel",
            "_options_labels": [
              "CPU",
              "AUTO"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "RadioButtonsView",
            "description": "Device:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_a5dd9b3899154607994fceab27d65f21",
            "style": "IPY_MODEL_9cb470776e0443e39b5b40b1fd893b36"
          }
        },
        "a5dd9b3899154607994fceab27d65f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb470776e0443e39b5b40b1fd893b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8634ab4037fd4f04a43ef5ad2f19ecbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "RadioButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "RadioButtonsModel",
            "_options_labels": [
              "INT8"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "RadioButtonsView",
            "description": "Model to run:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_28deaea0466d451bb3216c7974ff40f8",
            "style": "IPY_MODEL_0063decb3d3b4d9d8e8f2a561711acff"
          }
        },
        "28deaea0466d451bb3216c7974ff40f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0063decb3d3b4d9d8e8f2a561711acff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}